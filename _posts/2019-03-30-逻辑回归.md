---
layout: post
title: 逻辑回归模型
categories: [机器学习]
description: 机器学习
keywords: 机器学习
mathjax: true
---

主要介绍逻辑回归。

# LR(Logistic Regression)逻辑回归

## Logistic函数
$$h(t) = \cfrac{1}{1 + e^{-t}}$$

标准Logistic函数

![c300](/images/15539585398488.jpg)

性质
- 通分
$$h(t) = \cfrac{1}{1 + e^{-t}} = \cfrac{e^{t}}{1 + e^{t}}$$


- “对称”

$$1-h(t)=h(-t) \implies 关于点(0,0.5)中心对称$$

- 导数

$$h'(t)=h(t)\times(1-h(t))$$

## 逻辑回归

- 确定损失函数：

$$h_\theta(x) = \sigma(\theta^Tx)=\cfrac{1}{1 + e^{- \theta^Tx}}$$

- 概率表示

![-w290](/images/15539589129054.jpg)

上式简化为：
![-w371](/images/15539589215674.jpg)

似然函数：
![-w425](/images/15539589317301.jpg)

便于计算，取对数似然函数，并且取相反数，转变为求解最小值：

$$J(\theta) = - \sum_i^m \left(y^{(i)} \log( h_\theta(x^{(i)}) ) + (1 - y^{(i)}) \log( 1 - h_\theta(x^{(i)}) ) \right)$$

* 梯度计算:

$$\nabla_\theta =\cfrac{\partial J(\theta)}{\partial \theta} = \sum_i x^{(i)} (h_\theta(x^{(i)}) - y^{(i)})$$


梯度下降:

$$\theta := \theta - \alpha(\nabla_\theta)$$

注意，当采用随机梯度下降方法时候，梯度可简化为：

$$\nabla_\theta =\cfrac{\partial J(\theta)}{\partial \theta} = x(h_\theta(x) - y)$$

其中$(x,y)$为随机样本点

则最终梯度下降可表示为

$$
\begin{split}
\theta_j := \theta_j - \alpha(\nabla_{\theta_j}) = \theta + \alpha \cdot x_j(y_j-h_\theta(x_j)) &\end{split}
$$

$$
\begin{split}
b_j := b_j - \beta(\nabla_{b_j}) = b + \beta \cdot (y_j-h_\theta(x_j)) 
\end{split}
$$

其中, $j$为列向量维度





