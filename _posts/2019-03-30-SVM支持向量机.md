---
layout: post
title: SVM支持向量机
categories: [机器学习]
description: 机器学习
keywords: 机器学习
mathjax: true
---

总结机器学习中的支持向量机。


# SVM支持向量机
## 间隔及支持向量

样本空间超平面

$$
\begin{equation}
\boldsymbol{w}^T \cdot x + b=0
\end{equation}
$$

点$(x,y)$到超平面的距离

$$
\begin{equation}
r=\cfrac{|\boldsymbol{w}^T \cdot x + b|}{||\boldsymbol{w}||}
\end{equation}
$$

间隔margin：距离划分超平面最近的样本到划分超平面距离的两倍

$$
\begin{equation}
\gamma = min{\cfrac{2}{||\boldsymbol{w}||}}
\end{equation}
$$

![-c397](/images/15539576870840.jpg)

$$
\begin{equation}
\max \quad \frac{2}{\Vert \boldsymbol{w} \Vert} \\ 
s.t. \quad y_i(\boldsymbol{w}^T x_i + b) \geqslant 0 ,\quad i=1,2,...,m
\end{equation}
$$

由于对 $(w, b)$ 的放缩不影响解，为了简化优化问题，我们约束 $(w, b)$ 使得$w^T\cdot x + b = 1$:

$$
\begin{equation}
r=\cfrac{|r\boldsymbol{w}^T \cdot x + rb|}{||\boldsymbol{rw}||} \\ 
= \cfrac{|\boldsymbol{w}^T \cdot x + b|}{||\boldsymbol{w}||}
\end{equation}
$$

优化目标简化为：

$$
\begin{equation}
\min_{\boldsymbol{w}, b} \quad \frac{\Vert \boldsymbol{w} \Vert}{2} \\ 
s.t. \quad y_i(\boldsymbol{w}^T x_i + b) \geqslant 1 ,\quad i=1,2,...,m
\end{equation}
$$

这就是svm的基本型

## 对偶问题
svm的基本型的优化可以使用拉格朗日乘子法解决。

设拉格朗日函数，不等式约束的 Lagrange Multiplier 需满足的条件$\alpha_i\geqslant 0$

$$
\begin{equation}
L(w, b, \alpha) = \cfrac12||\boldsymbol{w}|| + \sum \alpha_i\cdot (1-y_i(\boldsymbol{w}^T x_i + b)) 
\end{equation}
$$

对$w, b$求导得:

$$
\begin{equation}
\boldsymbol{w}=\sum_{i=1}^{m}\alpha_iy_ix_i \\
\sum_{i=1}^{m}\alpha_iy_i=0
\end{equation} 
$$

代入


