---
layout: post
title: LDA线性判别分析
categories: [机器学习]
description: 机器学习
keywords: 机器学习
mathjax: true
---

本文介绍LDA线性判别分析。

# LDA线性判别分析
LDA线性判别分析（Linear Discriminant Analysis, 以下简称LDA）是一种监督学习的降维技术，也就是说它的数据集的每个样本是有类别输出的。PCA是不考虑样本类别输出的无监督降维技术。
**投影后类内方差最小，类间方差最大**

![-w642](media/15506514805065/15506546754093.jpg)

## LDA原理

数据集：$D=\{(x_1,y_1), (x_2,y_2), ...,((x_m,y_m))\}$
任意样本：$x_i$为$n$维向量，$y_i\in \{0,1\}$
第$j$类样本的个数: $N_j(j=0,1)$
第$j$类样本的均值向量: $μ_j(j=0,1)$
第$j$类样本的协方差矩阵（严格说是缺少分母部分的协方差矩阵）: $\sum_j(j=0,1)$

$$
\begin{equation}
\mu_j = \frac{1}{N_j}\sum\limits_{x \in X_j}x\;\;(j=0,1) \\
\end{equation}
$$

$$
\begin{equation}
\Sigma_j = \sum\limits_{x \in X_j}(x-\mu_j)(x-\mu_j)^T\;\;(j=0,1)
\end{equation}
$$

由于是两类数据，因此我们只需要将数据投影到一条直线上即可。
投影直线:向量$w$,
任意一个样本本: $x_i$
投影:$w^Tx_i$
两个类别的中心点在直线w的投影:$w^Tμ_0$和$w^Tμ_1$。

优化目标：

$$
\begin{equation}
\underbrace{arg\;max}_w\;\;J(w) 
= \frac{||w^T\mu_0-w^T\mu_1||_2^2}{w^T\Sigma_0w+w^T\Sigma_1w}  = \frac{w^T(\mu_0-\mu_1)(\mu_0-\mu_1)^Tw}{w^T(\Sigma_0+\Sigma_1)w}
\end{equation}
$$

我们一般定义类内散度矩阵$S_w$为：
$$
\begin{equation}
S_w = \Sigma_0 + \Sigma_1 = \sum\limits_{x \in X_0}(x-\mu_0)(x-\mu_0)^T + \sum\limits_{x \in X_1}(x-\mu_1)(x-\mu_1)^T
\end{equation}
$$
同时定义类间散度矩阵$S_b$为：
$$
\begin{equation}
S_b = (\mu_0-\mu_1)(\mu_0-\mu_1)^T
\end{equation}
$$
这样我们的优化目标重写为：
$$
\begin{equation}
\underbrace{arg\;max}_w\;\;J(w) = \frac{w^TS_bw}{w^TS_ww}
\end{equation}
$$


由于$w$的缩放不会影响最终解，可增加约束$|w^TS_ww|=1$,加入拉格朗日函数后得到：

$$
\begin{equation}
S_bw=\lambda S_ww \implies S_w^{-1}S_bw=\lambda w
\end{equation}
$$
$w$为$S_w^{-1}S_b$的特征向量,求解即可得解。

进一步简化问题，简化问题，因此$S_bw=S_b = (\mu_0-\mu_1)(\mu_0-\mu_1)^T$,其方向始终为$(\mu_2-\mu_1)$ ，故可以用 $\lambda (\mu_2-\mu_1)$ 来表示，因此我们可以得到：

$$
\begin{equation}
w=S_w^{-1}(\mu_0-\mu_1)
\end{equation}
$$