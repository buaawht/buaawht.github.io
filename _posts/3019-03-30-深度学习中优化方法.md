---
layout: post
title: 深度学习中优化方法
categories: [深度学习]
description: 深度学习
keywords: 深度学习
mathjax: true
---

总结深度学习中主要的优化方法。

# 深度学习中优化方法
## 1. 梯度下降法
### 1.1 梯度下降算法变种
- Batch gradient descent BGD
- Stochastic gradient descent SGD
- Mini-batch gradient descent MBGD

### 1.2 梯度下降优化算法
- 指数加权平均（Exponentially weighted average）

$$
\begin{array}{lcl}
V_t &=& \beta V_{t-1} + (1-\beta \theta_t )\\
V_t &=& \frac{V_t}{1 - \beta^t}
\end{array}
$$

- Momentum

$$
\begin{array}{lcl}
v_t = \gamma v_{t-1} + \eta \nabla_{\theta}J(\theta)\\
\theta = \theta - v_t
\end{array}
$$
- Nesterov Momentum

$$
\begin{array}{lcl}
v_t = \gamma v_{t-1} + \eta \nabla_{\theta}J(\theta-\gamma v_{t-1})\\
\theta = \theta - v_t
\end{array}
$$

- Adagrad

$$
\theta_{t+1,i} = \theta_{t,i} - \frac{\eta }{\sqrt{G_{t,ii} + \epsilon}}\nabla_{\theta}J(\theta_{t,i})
$$

- RMSprop
未统一单位
$$
G_t \leftarrow E[g^2]_t=\gamma E[g^2]_{t-1} + (1-\gamma)g^2_t \\\theta_{t+1} = \theta_{t} - \frac{\eta }{\sqrt{G_t + \epsilon}}g_t
$$


- Adadelta
统一单位


- Adam

$$
E[g]_t=\beta_1 E[g]_{t-1} + (1-\beta_1)g_t \\
E[g^2]_t=\beta_2 E[g^2]_{t-1} + (1-\beta_2)g^2_t \\
$$

增加修正项：
$$
\hat{E[g]_t} = \frac{E[g]_t}{1-\beta_1} \\
\hat{E[g^2]_t} = \frac{E[g^2]_t}{1-\beta_2} \\
$$
最终迭代：
$$
\theta_{t+1} = \theta_{t}  - \frac{\eta }{\sqrt{\hat{E[g^2]_t} + \epsilon}}\hat{E[g]_t}
$$
[动态图1](https://img-blog.csdn.net/20170721213552012?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2h1emZhbg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
[动态图2](https://img-blog.csdn.net/20170721213727999?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2h1emZhbg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

![](/images/15541873844384.jpg)

----

## 参考文献
- https://blog.csdn.net/u012328159/article/details/80311892
- https://blog.csdn.net/shuzfan/article/details/75675568
- https://zhuanlan.zhihu.com/p/31630368
- http://www.lunarnai.cn/2017/05/14/gradient-descent/
